# âœ… Linear Model Assumption Checks

---

## ğŸ”– Severity Levels

### ğŸ”´ High â€” Must fix before trusting model.

### ğŸŸ  Moderate â€” Should fix for valid inference or cleaner prediction.

### ğŸŸ¡ Low/Informative â€” Worth noting; often fixable with robust stats or larger data.

---

## âœ… 1. Normality of Residuals

### ğŸ“Œ Why It Matters:

Normal residuals allow for valid confidence intervals and hypothesis testing. If this assumption fails, inference may be misleading.

### ğŸ” How to Check It:

**Visual:**

- Q-Q Plot (Quantile-Quantile)
- Histogram of residuals

**Statistical:**

- Shapiro-Wilk Test
- Dâ€™Agostino and Pearsonâ€™s Test (`scipy.stats.normaltest`)
- Anderson-Darling Test (stricter, emphasizes tails)

### âœ… Pass Criteria:

- p-value > 0.05 â†’ fail to reject normality
- Q-Q plot: residuals should fall roughly along the diagonal

### âŒ If Test Fails:

**Severity:**

- ğŸŸ¡ Moderate
- Mainly impacts inference, not predictions. Critical for confidence intervals & p-values.

**Recommendation:**

- Consider applying transformations to the target (e.g., log, square root)
- Use robust regression models or non-parametric methods

---

## âœ… 2. Multicollinearity

### ğŸ“Œ Why It Matters:

Multicollinearity inflates coefficient variance, making estimates unstable and hard to interpret.

### ğŸ” How to Check It:

- Variance Inflation Factor (VIF)
- Feature correlation matrix (visual)

### âœ… Pass Criteria:

- VIF < 5 (acceptable), < 10 (tolerable)

### âŒ If Test Fails:

**Severity:**

- ğŸŸ  Moderate to High
- Inflates variance, can destabilize coefficients, reduce interpretability, and increase overfitting risk.

**Recommendation:**

- Remove or combine correlated predictors
- Use PCA or regularization (Ridge/Lasso)

---

## âœ… 3. Linearity

### ğŸ“Œ Why It Matters:

Linear models assume a linear relationship between predictors and the target variable.

### ğŸ” How to Check It:

- Residuals vs Fitted Plot
- Add non-linear features (e.g., polynomials) and evaluate performance

### âœ… Pass Criteria:

- Residuals scattered randomly around zero
- No visible patterns or curves in residual plot

### âŒ If Test Fails:

**Severity:**

- ğŸ”´ High
- Violates the foundational assumption. Predictions & inference both become unreliable.

**Recommendation:**

- Add polynomial or interaction terms
- Switch to non-linear models (e.g., trees, splines)

---

## âœ… 4. Homoscedasticity

### ğŸ“Œ Why It Matters:

Linear regression assumes constant variance of residuals across levels of fitted values. Violation implies inefficient estimates.

### ğŸ” How to Check It:

- Residuals vs Fitted Plot
- Breusch-Pagan Test

### âœ… Pass Criteria:

- Breusch-Pagan p-value > 0.05
- Residuals show no funnel shape

### âŒ If Test Fails:

**Severity:**

- ğŸŸ¡ Moderate
- Affects efficiency of estimates. Standard errors may be inaccurate or inefficient. Inference may be misleading.

**Recommendation:**

- Use heteroscedasticity-robust standard errors
- Transform the dependent variable

---

## âœ… 5. Independence

### ğŸ“Œ Why It Matters:

Residuals should be independent. Violation (especially in time series) leads to biased inference.

### ğŸ” How to Check It:

- Durbin-Watson Test
- Autocorrelation Function (ACF) Plot

### âœ… Pass Criteria:

- Durbin-Watson âˆˆ [1.5, 2.5]

### âŒ If Test Fails:

**Severity:**

- ğŸ”´ High (in time/sequence data)
- Causes misleading standard errors and invalid inference. Often overlooked but severe in time series.

**Recommendation:**

- Model residual correlation (e.g., use ARIMA or GLS)
- Add lagged predictors or use time series models

---

# ğŸ§ª Additional Assumption Checks (Planned / In Progress)

## ğŸ” 6. Influence

### ğŸ“Œ Why It Matters:

Influential points disproportionately affect model coefficients. They're not just outliers â€” they can bend the model to their will.

### ğŸ” How to Check It:

- Cookâ€™s Distance
- Hat Values / Leverage Scores
- DFBETAs (change in coefficients if point is removed)

### âœ… Pass Criteria:

- No points with Cook's Distance > 4/n or > 1 (rule of thumb)
- Leverage values within reasonable bounds (typically < 2p/n)

### âŒ If Test Fails:

**Severity:**

- ğŸ”´ High
- Can distort the entire model. A few points can shift coefficients dramatically.

**Recommendation:**

- Investigate high-leverage observations
- Consider model re-fit without those points
- Use robust regression (e.g., Huber, RANSAC)

---

## ğŸ” 7. Outliers

### ğŸ“Œ Why It Matters:

Extreme residuals can distort model fit and invalidate inference â€” even if they're not high-leverage points.

### ğŸ” How to Check It:

- Standardized or studentized residuals
- Bonferroni-adjusted p-values for residual tests
- Influence plots (residual vs leverage)

### âœ… Pass Criteria:

- No residuals beyond Â±3 standard deviations
- Bonferroni-corrected outlier test not significant

### âŒ If Test Fails:

**Severity:**

- ğŸŸ  Moderate to High
- Impacts fit, error metrics, and interpretability. Context-dependent â€” extreme values may be valid. More serious in small data.

**Recommendation:**

- Investigate if outliers are data entry errors or valid extremes
- Consider transformation or robust methods
- Flag for domain expert review

---

## ğŸ” 8. Missingness

### ğŸ“Œ Why It Matters:

Patterns of missing data can introduce bias, especially if not missing completely at random (MCAR).

### ğŸ” How to Check It:

- Count and percentage of missing values
- Heatmap of missingness
- Littleâ€™s MCAR test (advanced)
- Missingness vs target correlation

### âœ… Pass Criteria:

- <5% missing overall, no strong pattern by feature
- MCAR or ignorable MAR assumed

### âŒ If Test Fails:

**Severity:**

- ğŸŸ  Moderate to High
- Can bias results, especially if not Missing Completely at Random (MCAR). The impact depends on extent & pattern.

**Recommendation:**

- Impute missing data using appropriate method (mean, MICE, KNN)
- Use models that handle missingness (e.g., tree-based, Bayesian)
- Avoid dropping rows unless missingness is random and rare
